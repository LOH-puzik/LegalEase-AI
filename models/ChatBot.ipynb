{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "11.8\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376bf31da9204c11bdd6b798ea42702a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78fd3e43d14f4cbd8852e1db7a56bf00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bf8d9b5b6e49ba8c95453dcd9d8ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-large\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-large\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_text = \"how much jail time will get for smoking weed in public?\"\n",
    "# input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(input_ids, num_beams=5, \n",
    "#                         max_new_tokens=50, early_stopping=True,\n",
    "#                         no_repeat_ngram_size=2)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack.document_stores import PineconeDocumentStore\n",
    "\n",
    "# document_store = PineconeDocumentStore(\n",
    "#     api_key='',\n",
    "#     environment = 'us-west4-gcp-free',\n",
    "#     index='document',\n",
    "#     similarity=\"dot_product\",\n",
    "#     embedding_dim=768\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (C:/Users/Dhia/.cache/huggingface/datasets/wikipedia/20220301.simple/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '186',\n",
       " 'url': 'https://simple.wikipedia.org/wiki/Dublin',\n",
       " 'title': 'Dublin',\n",
       " 'text': 'Dublin () is the capital of the Republic of Ireland, and the biggest city on the island of Ireland. In 2011, there were over 1.1 million people living in the Greater Dublin Area.\\n\\nDublin was built by the Vikings upon the river Liffey. The river divides the city into two parts, North Dublin and South Dublin.\\n\\nMany famous writers lived in Dublin. Oscar Wilde and George Bernard Shaw were born in Dublin. James Joyce is probably Dublin\\'s best known and most international writer.\\n\\nDublin is home to Ireland\\'s largest stadium for all sports, Croke Park. It can hold up to 85,000 people. Croke Park is the usual venue for all Ireland hurling and football finals. The Aviva Stadium hosts rugby and soccer.\\n\\nNotes\\n\\nReferences\\n\\nOther websites \\n\\n \"The Reflecting City\"\\n Dublin GDP stats \\n WikiSatellite view of Dublin at WikiMapia\\n The Dublin Community Blog\\n Satellite map of Dublin - Including local geographic features.\\n Dublin - Tourist information.\\n\\n \\nCounty towns in Ireland'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Encoding 'cl100k_base'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tiktoken.encoding_for_model('gpt-3.5-turbo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "# Create the length function\n",
    "def tiktoken_len(text):\n",
    "    tokens = tokenizer.encode(\n",
    "        text,\n",
    "        disallowed_special=()\n",
    "    )\n",
    "    return len(tokens)\n",
    "\n",
    "tiktoken_len(\"hello I am a chunk of text and using the tiktoken_len function \"\n",
    "             \"we can find the length of this chunk of text in tokens no no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=20,\n",
    "    length_function=tiktoken_len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alan Mathison Turing OBE FRS (London, 23 June 1912 – Wilmslow, Cheshire, 7 June 1954) was an English mathematician and computer scientist. He was born in Maida Vale, London.\\n\\nEarly life and family \\nAlan Turing was born in Maida Vale, London on 23 June 1912. His father was part of a family of merchants from Scotland. His mother, Ethel Sara, was the daughter of an engineer.\\n\\nEducation \\nTuring went to St. Michael\\'s, a school at 20 Charles Road, St Leonards-on-sea, when he was five years old.\\n\"This is only a foretaste of what is to come, and only the shadow of what is going to be.” – Alan Turing.\\n\\nThe Stoney family were once prominent landlords, here in North Tipperary. His mother Ethel Sara Stoney (1881–1976) was daughter of Edward Waller Stoney (Borrisokane, North Tipperary) and Sarah Crawford (Cartron Abbey, Co. Longford); Protestant Anglo-Irish gentry.\\n\\nEducated in Dublin at Alexandra School and College; on October 1st 1907 she married Julius Mathison Turing, latter son of Reverend John Robert Turing and Fanny Boyd, in Dublin. Born on June 23rd 1912, Alan Turing would go on to be regarded as one of the greatest figures of the twentieth century.',\n",
       " 'A brilliant mathematician and cryptographer Alan was to become the founder of modern-day computer science and artificial intelligence; designing a machine at Bletchley Park to break secret Enigma encrypted messages used by the Nazi German war machine to protect sensitive commercial, diplomatic and military communications during World War 2. Thus, Turing made the single biggest contribution to the Allied victory in the war against Nazi Germany, possibly saving the lives of an estimated 2 million people, through his effort in shortening World War II.\\n\\nIn 2013, almost 60 years later, Turing received a posthumous Royal Pardon from Queen Elizabeth II. Today, the “Turing law” grants an automatic pardon to men who died before the law came into force, making it possible for living convicted gay men to seek pardons for offences now no longer on the statute book.\\n\\nAlas, Turing accidentally or otherwise lost his life in 1954, having been subjected by a British court to chemical castration, thus avoiding a custodial sentence. He is known to have ended his life at the age of 41 years, by eating an apple laced with cyanide.\\n\\nCareer \\nTuring was one of the people who worked on the first computers. He created the theoretical  Turing machine in 1936. The machine was imaginary, but it included the idea of a computer program.\\n\\nTuring was interested in artificial intelligence. He proposed the Turing test, to say when a machine could be called \"intelligent\". A computer could be said to \"think\" if a human talking with it could not tell it was a machine.',\n",
       " 'During World War II, Turing worked with others to break German ciphers (secret messages). He  worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain\\'s codebreaking centre that produced Ultra intelligence.\\nUsing cryptanalysis, he helped to break the codes of the Enigma machine. After that, he worked on other German codes.\\n\\nFrom 1945 to 1947, Turing worked on the design of the ACE (Automatic Computing Engine) at the National Physical Laboratory. He presented a paper on 19 February 1946. That paper was \"the first detailed design of a stored-program computer\". Although it was possible to build ACE, there were delays in starting the project. In late 1947 he returned to Cambridge for a sabbatical year. While he was at Cambridge, the Pilot ACE was built without him. It ran its first program on 10\\xa0May 1950.\\n\\nPrivate life \\nTuring was a homosexual man. In 1952, he admitted having had sex with a man in England. At that time, homosexual acts were illegal. Turing was convicted. He had to choose between going to jail and taking hormones to lower his sex drive. He decided to take the hormones. After his punishment, he became impotent. He also grew breasts.\\n\\nIn May 2012, a private member\\'s bill was put before the House of Lords to grant Turing a statutory pardon. In July 2013, the government supported it. A royal pardon was granted on 24 December 2013.\\n\\nDeath \\nIn 1954, Turing died from cyanide poisoning. The cyanide came from either an apple which was poisoned with cyanide, or from water that had cyanide in it. The reason for the confusion is that the police never tested the apple for cyanide. It is also suspected that he committed suicide.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = text_splitter.split_text(data[6]['text'])[:3]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "texts = [\n",
    "    'this is the first chunk of text',\n",
    "    'then another second chunk of text is here',\n",
    "    'and finally a third chunk of text'\n",
    "]\n",
    "\n",
    "embed = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2').to(\"cuda\")\n",
    "embeddings = embed.encode(texts)\n",
    "len(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "\n",
    "YOUR_API_KEY = ''\n",
    "YOUR_ENV = 'us-west4-gcp-free'\n",
    "\n",
    "index_name = 'doc1'\n",
    "pinecone.init(\n",
    "    api_key=YOUR_API_KEY,\n",
    "    environment=YOUR_ENV\n",
    ")\n",
    "\n",
    "if index_name not in pinecone.list_indexes():\n",
    "    # Create a new index\n",
    "    pinecone.create_index(\n",
    "        name=index_name,\n",
    "        metric='dotproduct',\n",
    "        dimension=len(res[0]) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.1,\n",
       " 'namespaces': {'': {'vector_count': 28433}},\n",
       " 'total_vector_count': 28433}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21258979867b42e3b2202769416434ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from uuid import uuid4\n",
    "\n",
    "batch_limit = 100\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for i, record in enumerate(tqdm(data)):\n",
    "    # Get metadata fields for this record\n",
    "    metadata = {\n",
    "        'wiki-id': str(record['id']),\n",
    "        'source': record['url'],\n",
    "        'title': record['title']\n",
    "    }\n",
    "    # Create chunks from the record text\n",
    "    record_texts = text_splitter.split_text(record['text'])\n",
    "    # Vreate individual metadata dicts for each chunk\n",
    "    record_metadatas = [{\n",
    "        \"chunk\": j, \"text\": text, **metadata\n",
    "    } for j, text in enumerate(record_texts)]\n",
    "    # Append these to current batches\n",
    "    texts.extend(record_texts)\n",
    "    metadatas.extend(record_metadatas)\n",
    "    # If we have reached the batch_limit we can add texts\n",
    "    if len(texts) >= batch_limit:\n",
    "        ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "        embeds = [embed_vector.tolist() for embed_vector in embed.encode(texts)] # Adjusted dimension\n",
    "        index.upsert(vectors=zip(ids, embeds, metadatas))\n",
    "        texts = []\n",
    "        metadatas = []\n",
    "\n",
    "if len(texts) > 0:\n",
    "    ids = [str(uuid4()) for _ in range(len(texts))]\n",
    "    embeds = [embed_vector.tolist() for embed_vector in embed.encode(texts)] # Adjusted dimension\n",
    "    index.upsert(vectors=zip(ids, embeds, metadatas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.2,\n",
       " 'namespaces': {'': {'vector_count': 56866}},\n",
       " 'total_vector_count': 56866}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "def embed_query(query):\n",
    "    query = embed.encode([query])\n",
    "    return query\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# Switch back to normal index for langchain\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute '_composed_schemas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwho was Benito Mussolini?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m vectorstore\u001b[39m.\u001b[39;49msimilarity_search(\n\u001b[0;32m      4\u001b[0m     query,  \u001b[39m# our search query\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m     k\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m  \u001b[39m# return 3 most relevant docs\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\langchain\\vectorstores\\pinecone.py:150\u001b[0m, in \u001b[0;36mPinecone.similarity_search\u001b[1;34m(self, query, k, filter, namespace, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m query_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embedding_function(query)\n\u001b[0;32m    149\u001b[0m docs \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 150\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index\u001b[39m.\u001b[39;49mquery(\n\u001b[0;32m    151\u001b[0m     [query_obj],\n\u001b[0;32m    152\u001b[0m     top_k\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m    153\u001b[0m     include_metadata\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    154\u001b[0m     namespace\u001b[39m=\u001b[39;49mnamespace,\n\u001b[0;32m    155\u001b[0m     \u001b[39mfilter\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mfilter\u001b[39;49m,\n\u001b[0;32m    156\u001b[0m )\n\u001b[0;32m    157\u001b[0m \u001b[39mfor\u001b[39;00m res \u001b[39min\u001b[39;00m results[\u001b[39m\"\u001b[39m\u001b[39mmatches\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    158\u001b[0m     metadata \u001b[39m=\u001b[39m res[\u001b[39m\"\u001b[39m\u001b[39mmetadata\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\utils\\error_handling.py:17\u001b[0m, in \u001b[0;36mvalidate_and_convert_errors.<locals>.inner_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     15\u001b[0m Config\u001b[39m.\u001b[39mvalidate()  \u001b[39m# raises exceptions in case of invalid config\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     18\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     19\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ProtocolError):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\index.py:449\u001b[0m, in \u001b[0;36mIndex.query\u001b[1;34m(self, vector, id, queries, top_k, namespace, filter, include_values, include_metadata, sparse_vector, **kwargs)\u001b[0m\n\u001b[0;32m    439\u001b[0m sparse_vector \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_sparse_values_arg(sparse_vector)\n\u001b[0;32m    440\u001b[0m args_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parse_non_empty_args([(\u001b[39m'\u001b[39m\u001b[39mvector\u001b[39m\u001b[39m'\u001b[39m, vector),\n\u001b[0;32m    441\u001b[0m                                         (\u001b[39m'\u001b[39m\u001b[39mid\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mid\u001b[39m),\n\u001b[0;32m    442\u001b[0m                                         (\u001b[39m'\u001b[39m\u001b[39mqueries\u001b[39m\u001b[39m'\u001b[39m, queries),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    447\u001b[0m                                         (\u001b[39m'\u001b[39m\u001b[39minclude_metadata\u001b[39m\u001b[39m'\u001b[39m, include_metadata),\n\u001b[0;32m    448\u001b[0m                                         (\u001b[39m'\u001b[39m\u001b[39msparse_vector\u001b[39m\u001b[39m'\u001b[39m, sparse_vector)])\n\u001b[1;32m--> 449\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_vector_api\u001b[39m.\u001b[39mquery(\n\u001b[0;32m    450\u001b[0m     QueryRequest(\n\u001b[0;32m    451\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs_dict,\n\u001b[0;32m    452\u001b[0m         _check_type\u001b[39m=\u001b[39m_check_type,\n\u001b[0;32m    453\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS}\n\u001b[0;32m    454\u001b[0m     ),\n\u001b[0;32m    455\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m kwargs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m _OPENAPI_ENDPOINT_PARAMS}\n\u001b[0;32m    456\u001b[0m )\n\u001b[0;32m    457\u001b[0m \u001b[39mreturn\u001b[39;00m parse_query_response(response, vector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mid\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\client\\api_client.py:776\u001b[0m, in \u001b[0;36mEndpoint.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    765\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    766\u001b[0m     \u001b[39m\"\"\" This method is invoked when endpoints are called\u001b[39;00m\n\u001b[0;32m    767\u001b[0m \u001b[39m    Example:\u001b[39;00m\n\u001b[0;32m    768\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m \n\u001b[0;32m    775\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 776\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallable(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\client\\api\\vector_operations_api.py:716\u001b[0m, in \u001b[0;36mVectorOperationsApi.__init__.<locals>.__query\u001b[1;34m(self, query_request, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m_host_index\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    714\u001b[0m kwargs[\u001b[39m'\u001b[39m\u001b[39mquery_request\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[0;32m    715\u001b[0m     query_request\n\u001b[1;32m--> 716\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_with_http_info(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\client\\api_client.py:838\u001b[0m, in \u001b[0;36mEndpoint.call_with_http_info\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    834\u001b[0m     header_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_client\u001b[39m.\u001b[39mselect_header_content_type(\n\u001b[0;32m    835\u001b[0m         content_type_headers_list)\n\u001b[0;32m    836\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mheader\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m header_list\n\u001b[1;32m--> 838\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi_client\u001b[39m.\u001b[39;49mcall_api(\n\u001b[0;32m    839\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mendpoint_path\u001b[39;49m\u001b[39m'\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mhttp_method\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    840\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mpath\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    841\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    842\u001b[0m     params[\u001b[39m'\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    843\u001b[0m     body\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    844\u001b[0m     post_params\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mform\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    845\u001b[0m     files\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mfile\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    846\u001b[0m     response_type\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mresponse_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    847\u001b[0m     auth_settings\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mauth\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    848\u001b[0m     async_req\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39masync_req\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    849\u001b[0m     _check_type\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_check_return_type\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    850\u001b[0m     _return_http_data_only\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_return_http_data_only\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    851\u001b[0m     _preload_content\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_preload_content\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    852\u001b[0m     _request_timeout\u001b[39m=\u001b[39;49mkwargs[\u001b[39m'\u001b[39;49m\u001b[39m_request_timeout\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    853\u001b[0m     _host\u001b[39m=\u001b[39;49m_host,\n\u001b[0;32m    854\u001b[0m     collection_formats\u001b[39m=\u001b[39;49mparams[\u001b[39m'\u001b[39;49m\u001b[39mcollection_format\u001b[39;49m\u001b[39m'\u001b[39;49m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\client\\api_client.py:413\u001b[0m, in \u001b[0;36mApiClient.call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[39m\"\"\"Makes the HTTP request (synchronous) and returns deserialized data.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[39mTo make an async_req request, set the async_req parameter.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[39m    then the method will return the response directly.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    412\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m async_req:\n\u001b[1;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__call_api(resource_path, method,\n\u001b[0;32m    414\u001b[0m                            path_params, query_params, header_params,\n\u001b[0;32m    415\u001b[0m                            body, post_params, files,\n\u001b[0;32m    416\u001b[0m                            response_type, auth_settings,\n\u001b[0;32m    417\u001b[0m                            _return_http_data_only, collection_formats,\n\u001b[0;32m    418\u001b[0m                            _preload_content, _request_timeout, _host,\n\u001b[0;32m    419\u001b[0m                            _check_type)\n\u001b[0;32m    421\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool\u001b[39m.\u001b[39mapply_async(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__call_api, (resource_path,\n\u001b[0;32m    422\u001b[0m                                                method, path_params,\n\u001b[0;32m    423\u001b[0m                                                query_params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m                                                _request_timeout,\n\u001b[0;32m    432\u001b[0m                                                _host, _check_type))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\client\\api_client.py:185\u001b[0m, in \u001b[0;36mApiClient.__call_api\u001b[1;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39m# body\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39mif\u001b[39;00m body:\n\u001b[1;32m--> 185\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msanitize_for_serialization(body)\n\u001b[0;32m    187\u001b[0m \u001b[39m# auth setting\u001b[39;00m\n\u001b[0;32m    188\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_params_for_auth(header_params, query_params,\n\u001b[0;32m    189\u001b[0m                             auth_settings, resource_path, method, body)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\client\\api_client.py:278\u001b[0m, in \u001b[0;36mApiClient.sanitize_for_serialization\u001b[1;34m(cls, obj)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[39m\"\"\"Prepares data for transmission before it is sent with the rest client\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[39mIf obj is None, return None.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[39mIf obj is str, int, long, float, bool, return directly.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39m:return: The serialized form of data.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, (ModelNormal, ModelComposed)):\n\u001b[0;32m    277\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m--> 278\u001b[0m         key: \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msanitize_for_serialization(val) \u001b[39mfor\u001b[39;00m key, val \u001b[39min\u001b[39;00m model_to_dict(obj, serialize\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mitems()\n\u001b[0;32m    279\u001b[0m     }\n\u001b[0;32m    280\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(obj, io\u001b[39m.\u001b[39mIOBase):\n\u001b[0;32m    281\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_file_data_and_close_file(obj)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\client\\model_utils.py:1666\u001b[0m, in \u001b[0;36mmodel_to_dict\u001b[1;34m(model_instance, serialize)\u001b[0m\n\u001b[0;32m   1664\u001b[0m                res\u001b[39m.\u001b[39mappend(v\u001b[39m.\u001b[39mvalue)\n\u001b[0;32m   1665\u001b[0m            \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1666\u001b[0m                res\u001b[39m.\u001b[39mappend(model_to_dict(v, serialize\u001b[39m=\u001b[39;49mserialize))\n\u001b[0;32m   1667\u001b[0m        result[attr] \u001b[39m=\u001b[39m res\n\u001b[0;32m   1668\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pinecone\\core\\client\\model_utils.py:1638\u001b[0m, in \u001b[0;36mmodel_to_dict\u001b[1;34m(model_instance, serialize)\u001b[0m\n\u001b[0;32m   1635\u001b[0m result \u001b[39m=\u001b[39m {}\n\u001b[0;32m   1637\u001b[0m model_instances \u001b[39m=\u001b[39m [model_instance]\n\u001b[1;32m-> 1638\u001b[0m \u001b[39mif\u001b[39;00m model_instance\u001b[39m.\u001b[39;49m_composed_schemas:\n\u001b[0;32m   1639\u001b[0m     model_instances\u001b[39m.\u001b[39mextend(model_instance\u001b[39m.\u001b[39m_composed_instances)\n\u001b[0;32m   1640\u001b[0m seen_json_attribute_names \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '_composed_schemas'"
     ]
    }
   ],
   "source": [
    "query = \"who was Benito Mussolini?\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", device_map=\"auto\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OPENAI_API_KEY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m RetrievalQA\n\u001b[0;32m      4\u001b[0m \u001b[39m# completion llm\u001b[39;00m\n\u001b[0;32m      5\u001b[0m llm \u001b[39m=\u001b[39m ChatOpenAI(\n\u001b[1;32m----> 6\u001b[0m     openai_api_key\u001b[39m=\u001b[39mOPENAI_API_KEY,\n\u001b[0;32m      7\u001b[0m     model_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpt-3.5-turbo\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m     temperature\u001b[39m=\u001b[39m\u001b[39m0.0\u001b[39m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     11\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(\n\u001b[0;32m     12\u001b[0m     llm\u001b[39m=\u001b[39mllm,\n\u001b[0;32m     13\u001b[0m     chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     14\u001b[0m     retriever\u001b[39m=\u001b[39mvectorstore\u001b[39m.\u001b[39mas_retriever()\n\u001b[0;32m     15\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'OPENAI_API_KEY' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Completion llm\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=generator,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "qa_with_sources = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_with_sources(query)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FROM THE BOOK \"NLP WITH TRANSFORMERS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de8fda87aba4fd9a16fc13b076f89e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/9.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e78653976274367a0730c882166a0d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/17.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93d4ba5040147269c4003092fbd97fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/21.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "domains = get_dataset_config_names(\"subjqa\")\n",
    "domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset subjqa/electronics to C:/Users/Dhia/.cache/huggingface/datasets/subjqa/electronics/1.1.0/2c12e496c4c675ab4a57ffb5d3f538f2e7b89793956e50da37126393ce23b6c6...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940521f469ee4d79bce52db9765c7fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb0cb60318514704b65cc95e75b69b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1295 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8560e1d9f8364d3cbb899a5331e1ac65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83a05590b20d45ffbc89dc2fcc30b1a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset subjqa downloaded and prepared to C:/Users/Dhia/.cache/huggingface/datasets/subjqa/electronics/1.1.0/2c12e496c4c675ab4a57ffb5d3f538f2e7b89793956e50da37126393ce23b6c6. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7935d087157a4306b4b0a498ea80b58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "subjqa = load_dataset(\"subjqa\", name=\"electronics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Bass is weak as expected', 'Bass is weak as expected, even with EQ adjusted up'], 'answer_start': [1302, 1302], 'answer_subj_level': [1, 1], 'ans_subj_score': [0.5083333253860474, 0.5083333253860474], 'is_ans_subjective': [True, True]}\n"
     ]
    }
   ],
   "source": [
    "print(subjqa[\"train\"][\"answers\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions in train: 1295\n",
      "Number of questions in test: 358\n",
      "Number of questions in validation: 255\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n",
    "\n",
    "for split, df in dfs.items():\n",
    "    print(f\"Number of questions in {split}: {df['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>answers.text</th>\n",
       "      <th>answers.answer_start</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>791</th>\n",
       "      <td>B005DKZTMG</td>\n",
       "      <td>Does the keyboard lightweight?</td>\n",
       "      <td>[this keyboard is compact]</td>\n",
       "      <td>[215]</td>\n",
       "      <td>I really like this keyboard.  I give it 4 star...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1159</th>\n",
       "      <td>B00AAIPT76</td>\n",
       "      <td>How is the battery?</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>I bought this after the first spare gopro batt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           title                        question                answers.text  \\\n",
       "791   B005DKZTMG  Does the keyboard lightweight?  [this keyboard is compact]   \n",
       "1159  B00AAIPT76             How is the battery?                          []   \n",
       "\n",
       "     answers.answer_start                                            context  \n",
       "791                 [215]  I really like this keyboard.  I give it 4 star...  \n",
       "1159                   []  I bought this after the first spare gopro batt...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_cols = [\"title\", \"question\", \"answers.text\", \n",
    "           \"answers.answer_start\", \"context\"]\n",
    "sample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this keyboard is compact'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\n",
    "end_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\n",
    "sample_df[\"context\"].iloc[0][start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c84fb61fce44e5b5f93dc7c49e7086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/107 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhia\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dhia\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29729619a0244d30bf7db70c16360118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/477 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58d42fdab31246bbbb22413df24b9f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7662c01374f846fdb87853014bd90599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_ckpt = \"deepset/minilm-uncased-squad2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How much music can this hold?\"\n",
    "context = \"\"\"An MP3 is about 1 MB/minute, so about 6000 hours depending on \\\n",
    "file size.\"\"\"\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>input_ids</th>\n",
       "      <td>101</td>\n",
       "      <td>2129</td>\n",
       "      <td>2172</td>\n",
       "      <td>2189</td>\n",
       "      <td>2064</td>\n",
       "      <td>2023</td>\n",
       "      <td>2907</td>\n",
       "      <td>1029</td>\n",
       "      <td>102</td>\n",
       "      <td>2019</td>\n",
       "      <td>...</td>\n",
       "      <td>2061</td>\n",
       "      <td>2055</td>\n",
       "      <td>25961</td>\n",
       "      <td>2847</td>\n",
       "      <td>5834</td>\n",
       "      <td>2006</td>\n",
       "      <td>5371</td>\n",
       "      <td>2946</td>\n",
       "      <td>1012</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token_type_ids</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attention_mask</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0     1     2     3     4     5     6     7    8     9   ...  \\\n",
       "input_ids       101  2129  2172  2189  2064  2023  2907  1029  102  2019  ...   \n",
       "token_type_ids    0     0     0     0     0     0     0     0    0     1  ...   \n",
       "attention_mask    1     1     1     1     1     1     1     1    1     1  ...   \n",
       "\n",
       "                  18    19     20    21    22    23    24    25    26   27  \n",
       "input_ids       2061  2055  25961  2847  5834  2006  5371  2946  1012  102  \n",
       "token_type_ids     1     1      1     1     1     1     1     1     1    1  \n",
       "attention_mask     1     1      1     1     1     1     1     1     1    1  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hide_input\n",
    "input_df = pd.DataFrame.from_dict(tokenizer(question, context), orient=\"index\")\n",
    "input_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how much music can this hold? [SEP] an mp3 is about 1 mb / minute, so about 6000 hours depending on file size. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82d6a9868d234d53bbe467a9fa309acc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-0.9862, -4.7750, -5.4025, -5.2378, -5.2863, -5.5117, -4.9819, -6.1880,\n",
      "         -0.9862,  0.2596, -0.2144, -1.7136,  3.7806,  4.8561, -1.0546, -3.9097,\n",
      "         -1.7374, -4.5944, -1.4278,  3.9949,  5.0391, -0.2018, -3.0193, -4.8549,\n",
      "         -2.3107, -3.5110, -3.5713, -0.9862]]), end_logits=tensor([[-0.9623, -5.4733, -5.0326, -5.1639, -5.4278, -5.5151, -5.1749, -4.6233,\n",
      "         -0.9623, -3.7855, -0.8715, -3.7745, -3.0161, -1.1780,  0.1758, -2.7365,\n",
      "          4.8934,  0.3046, -3.1761, -3.2762,  0.8937,  5.6606, -0.3623, -4.9554,\n",
      "         -3.2531, -0.0914,  1.6211, -0.9623]]), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_logits = outputs.start_logits\n",
    "end_logits = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs shape: torch.Size([1, 28])\n",
      "Start logits shape: torch.Size([1, 28])\n",
      "End logits shape: torch.Size([1, 28])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input IDs shape: {inputs.input_ids.size()}\")\n",
    "print(f\"Start logits shape: {start_logits.size()}\")\n",
    "print(f\"End logits shape: {end_logits.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How much music can this hold?\n",
      "Answer: 6000 hours\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "start_idx = torch.argmax(start_logits)  \n",
    "end_idx = torch.argmax(end_logits) + 1  \n",
    "answer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\n",
    "answer = tokenizer.decode(answer_span)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dhia\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\pipelines\\question_answering.py:325: UserWarning: topk parameter is deprecated, use top_k instead\n",
      "  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'score': 0.26516205072402954,\n",
       "  'start': 38,\n",
       "  'end': 48,\n",
       "  'answer': '6000 hours'},\n",
       " {'score': 0.22082941234111786,\n",
       "  'start': 16,\n",
       "  'end': 48,\n",
       "  'answer': '1 MB/minute, so about 6000 hours'},\n",
       " {'score': 0.10253491997718811,\n",
       "  'start': 16,\n",
       "  'end': 27,\n",
       "  'answer': '1 MB/minute'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\n",
    "pipe(question=question, context=context, topk=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.9068413972854614, 'start': 0, 'end': 0, 'answer': ''}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(question=\"Why is there no data?\", context=context, \n",
    "     handle_impossible_answer=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
